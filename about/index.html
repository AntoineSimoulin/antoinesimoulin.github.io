<!DOCTYPE html>
<html>
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>About</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=1ef6d3fffe">

    <link rel="icon" href="../content/images/size/w256h256/2021/11/logo-blog-1.png" type="image/png">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:type" content="website">
    <meta property="og:title" content="About">
    <meta property="og:description" content="I am currently completing my Ph.D. in Natural Language Processing at Paris
University [https://u-paris.fr/en/] in a joint program sponsored by Quantmetry
[https://www.quantmetry.com/]. I graduated from ENSTA Paris
[https://www.ensta-paris.fr/] in mathematical optimization and obtained a Master
(M2) from Ecole Polytechnique [https:">
    <meta property="og:url" content="AntoineSimoulin.github.io/about/">
    <meta property="og:image" content="https://static.ghost.org/v4.0.0/images/publication-cover.jpg">
    <meta property="article:published_time" content="2021-06-17T08:46:39.000Z">
    <meta property="article:modified_time" content="2022-01-13T09:14:49.000Z">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="About">
    <meta name="twitter:description" content="I am currently completing my Ph.D. in Natural Language Processing at Paris
University [https://u-paris.fr/en/] in a joint program sponsored by Quantmetry
[https://www.quantmetry.com/]. I graduated from ENSTA Paris
[https://www.ensta-paris.fr/] in mathematical optimization and obtained a Master
(M2) from Ecole Polytechnique [https:">
    <meta name="twitter:url" content="AntoineSimoulin.github.io/about/">
    <meta name="twitter:image" content="https://static.ghost.org/v4.0.0/images/publication-cover.jpg">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Antoine SIMOULIN">
    <meta name="twitter:site" content="@AntoineSimoulin">
    <meta property="og:image:width" content="7183">
    <meta property="og:image:height" content="2885">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "",
        "url": "AntoineSimoulin.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "AntoineSimoulin.github.io/content/images/size/w256h256/2021/11/logo-blog-1.png"
        }
    },
    "author": {
        "@type": "Person",
        "name": "Antoine SIMOULIN",
        "url": "AntoineSimoulin.github.io/author/antoine/",
        "sameAs": []
    },
    "headline": "About",
    "url": "AntoineSimoulin.github.io/about/",
    "datePublished": "2021-06-17T08:46:39.000Z",
    "dateModified": "2022-01-13T09:14:49.000Z",
    "description": "I am currently completing my Ph.D. in Natural Language Processing at Paris\nUniversity [https://u-paris.fr/en/] in a joint program sponsored by Quantmetry\n[https://www.quantmetry.com/]. I graduated from ENSTA Paris\n[https://www.ensta-paris.fr/] in mathematical optimization and obtained a Master\n(M2) from Ecole Polytechnique [https://www.polytechnique.edu/en] in Data\nScience.\n\nMy research focuses on building standalone sentence embeddings. I examine\nsentence encoder architectures, as well as train",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "AntoineSimoulin.github.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 5.4">
    <link rel="alternate" type="application/rss+xml" title="" href="../rss/index.rss">
    
    <script defer src="https://unpkg.com/@tryghost/sodo-search@~1.0.0/umd/sodo-search.min.js" data-sodo-search="AntoineSimoulin.github.io/" data-version="1.0.0" data-key="c2db0bdcd39be35a4e96686d03" crossorigin="anonymous"></script>
    <script defer src="../public/cards.min.js?v=1ef6d3fffe"></script>
    <link rel="stylesheet" type="text/css" href="../public/cards.min.css?v=1ef6d3fffe">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin="anonymous" referrerpolicy="no-referrer">
<style>
    
.collapsible {
  background-color: #7E2039;
  color: white;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
  margin-bottom: 10px;
}

.course::before {
  font-family: "Font Awesome 5 Free"; font-weight: 900; content: "\f19c";
  margin: 0 10px 0 0px;
}
    
.tp::before {
  font-family: "Font Awesome 5 Free"; font-weight: 900; content: "\f26c";
  margin: 0 10px 0 0px;
}

.collapsible:hover {
  background-color: #555;
}
    
.active {
  background-color: #7E2039;
}

.content {
  font-size: 15px;
  padding: 18px;
  display: none;
  transition: max-height 0.2s ease-out;
  background-color: #f1f1f1;
  margin-bottom: 10px;
}

.collapsible::after {
  font-family: "Font Awesome 5 Free";
  font-weight: 900;
  content: "\f067";
  float: right;
  margin-left: 5px;
  font-size: 13px;
}
    
.active:after {
  font-family: "Font Awesome 5 Free";
  content: "\f068";
}
    
button {
  background: #1976D2;
  border: none;
  color: #ffffff !important;
  cursor: pointer;
  outline: none;
  box-shadow: none;
  letter-spacing: 0.6px;
  font-size: 18px;
  border-radius: 2px;
  overflow: hidden;
  -webkit-backface-visibility: hidden;
  z-index: 2;
  text-transform: uppercase;
  font-size: 12px;
  font-weight: 600;
  position: relative;
  margin: 10px 0;
  box-shadow: 0 1px 2px rgba(0,0,0,0.12);
  padding: 0px 0.4rem;
  margin: 8px 8px 8px 0;
  height: 30px;
  display: inline-flex;
  align-items: center;
  justify-content: center;
}

</style><style>:root {--ghost-accent-color: #7E2039;}</style>

</head>
<body class="page-template page-about">

    <svg width="0" height="0" class="hidden">
        <symbol viewbox="0 0 24 24" xmlns="//www.w3.org/2000/svg" id="facebook">
            <title>Facebook icon</title>
            <path d="M23.998 12c0-6.628-5.372-12-11.999-12C5.372 0 0 5.372 0 12c0 5.988 4.388 10.952 10.124 11.852v-8.384H7.078v-3.469h3.046V9.356c0-3.008 1.792-4.669 4.532-4.669 1.313 0 2.686.234 2.686.234v2.953H15.83c-1.49 0-1.955.925-1.955 1.874V12h3.328l-.532 3.469h-2.796v8.384c5.736-.9 10.124-5.864 10.124-11.853z"></path>
        </symbol>
        <symbol viewbox="0 0 24 24" xmlns="//www.w3.org/2000/svg" id="github">
            <title>GitHub icon</title>
            <path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path>
        </symbol>
        <symbol xmlns="//www.w3.org/2000/svg" viewbox="0 0 24 24" id="linkedin">
            <title>LinkedIn icon</title>
            <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 0 1-2.063-2.065 2.064 2.064 0 1 1 2.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path>
        </symbol>
        <symbol viewbox="0 0 24 24" xmlns="//www.w3.org/2000/svg" id="mastodon">
            <title>Mastodon icon</title>
            <path d="M23.193 7.879c0-5.206-3.411-6.732-3.411-6.732C18.062.357 15.108.025 12.041 0h-.076c-3.068.025-6.02.357-7.74 1.147 0 0-3.411 1.526-3.411 6.732 0 1.192-.023 2.618.015 4.129.124 5.092.934 10.109 5.641 11.355 2.17.574 4.034.695 5.535.612 2.722-.15 4.25-.972 4.25-.972l-.09-1.975s-1.945.613-4.129.539c-2.165-.074-4.449-.233-4.799-2.891a5.499 5.499 0 0 1-.048-.745s2.125.52 4.817.643c1.646.075 3.19-.097 4.758-.283 3.007-.359 5.625-2.212 5.954-3.905.517-2.665.475-6.507.475-6.507zm-4.024 6.709h-2.497V8.469c0-1.29-.543-1.944-1.628-1.944-1.2 0-1.802.776-1.802 2.312v3.349h-2.483v-3.35c0-1.536-.602-2.312-1.802-2.312-1.085 0-1.628.655-1.628 1.944v6.119H4.832V8.284c0-1.289.328-2.313.987-3.07.68-.758 1.569-1.146 2.674-1.146 1.278 0 2.246.491 2.886 1.474L12 6.585l.622-1.043c.64-.983 1.608-1.474 2.886-1.474 1.104 0 1.994.388 2.674 1.146.658.757.986 1.781.986 3.07v6.304z"></path>
        </symbol>
        <symbol viewbox="0 0 24 24" xmlns="//www.w3.org/2000/svg" id="instagram">
            <title>Instagram icon</title>
            <path d="M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913a5.885 5.885 0 0 0 1.384 2.126A5.868 5.868 0 0 0 4.14 23.37c.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558a5.898 5.898 0 0 0 2.126-1.384 5.86 5.86 0 0 0 1.384-2.126c.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913a5.89 5.89 0 0 0-1.384-2.126A5.847 5.847 0 0 0 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227a3.81 3.81 0 0 1-.899 1.382 3.744 3.744 0 0 1-1.38.896c-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421a3.716 3.716 0 0 1-1.379-.899 3.644 3.644 0 0 1-.9-1.38c-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678a6.162 6.162 0 1 0 0 12.324 6.162 6.162 0 1 0 0-12.324zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405a1.441 1.441 0 0 1-2.88 0 1.44 1.44 0 0 1 2.88 0z"></path>
        </symbol>
        <symbol viewbox="0 0 24 24" xmlns="//www.w3.org/2000/svg" id="pinterest">
            <title>Pinterest icon</title>
            <path d="M12.017 0C5.396 0 .029 5.367.029 11.987c0 5.079 3.158 9.417 7.618 11.162-.105-.949-.199-2.403.041-3.439.219-.937 1.406-5.957 1.406-5.957s-.359-.72-.359-1.781c0-1.663.967-2.911 2.168-2.911 1.024 0 1.518.769 1.518 1.688 0 1.029-.653 2.567-.992 3.992-.285 1.193.6 2.165 1.775 2.165 2.128 0 3.768-2.245 3.768-5.487 0-2.861-2.063-4.869-5.008-4.869-3.41 0-5.409 2.562-5.409 5.199 0 1.033.394 2.143.889 2.741.099.12.112.225.085.345-.09.375-.293 1.199-.334 1.363-.053.225-.172.271-.401.165-1.495-.69-2.433-2.878-2.433-4.646 0-3.776 2.748-7.252 7.92-7.252 4.158 0 7.392 2.967 7.392 6.923 0 4.135-2.607 7.462-6.233 7.462-1.214 0-2.354-.629-2.758-1.379l-.749 2.848c-.269 1.045-1.004 2.352-1.498 3.146 1.123.345 2.306.535 3.55.535 6.607 0 11.985-5.365 11.985-11.987C23.97 5.39 18.592.026 11.985.026L12.017 0z"></path>
        </symbol>
        <symbol xmlns="//www.w3.org/2000/svg" viewbox="0 0 24 24" id="youtube">
            <title>YouTube icon</title>
            <path d="M23.495 6.205a3.007 3.007 0 0 0-2.088-2.088c-1.87-.501-9.396-.501-9.396-.501s-7.507-.01-9.396.501A3.007 3.007 0 0 0 .527 6.205a31.247 31.247 0 0 0-.522 5.805 31.247 31.247 0 0 0 .522 5.783 3.007 3.007 0 0 0 2.088 2.088c1.868.502 9.396.502 9.396.502s7.506 0 9.396-.502a3.007 3.007 0 0 0 2.088-2.088 31.247 31.247 0 0 0 .5-5.783 31.247 31.247 0 0 0-.5-5.805zM9.609 15.601V8.408l6.264 3.602z"></path>
        </symbol>
        <symbol viewbox="0 0 24 24" xmlns="//www.w3.org/2000/svg" id="twitter">
            <title>Twitter icon</title>
            <path d="M23.954 4.569a10 10 0 0 1-2.825.775 4.958 4.958 0 0 0 2.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 0 0-8.384 4.482C7.691 8.094 4.066 6.13 1.64 3.161a4.822 4.822 0 0 0-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 0 1-2.228-.616v.061a4.923 4.923 0 0 0 3.946 4.827 4.996 4.996 0 0 1-2.212.085 4.937 4.937 0 0 0 4.604 3.417 9.868 9.868 0 0 1-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 0 0 7.557 2.209c9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63a9.936 9.936 0 0 0 2.46-2.548l-.047-.02z"></path>
        </symbol>
    </svg>

    <div class="site-wrapper">

            <style>
    .site-head-left a, .site-head-right a {
        color: var(--color-base);
    }
</style>

<header class="site-head color">
    <div class="site-head-container">
        <a class="nav-burger" href="index.html#">
            <div class="hamburger hamburger--collapse" aria-label="Menu" role="button" aria-controls="navigation">
                <div class="hamburger-box">
                    <div class="hamburger-inner"></div>
                </div>
            </div>
        </a>
        <div class="site-head-left">
                <a class="site-head-logo" href="../index.html"></a>
        </div>
        <div class="site-head-center">
        </div>
        <div class="site-head-right">
            <ul class="nav">
            <li class="nav-home">
                <a href="../index.html">Home</a>
            </li>
            <li class="nav-about nav-current">
                <a href="index.html">About</a>
            </li>
            <li class="nav-projects">
                <a href="../projects/index.html">Projects</a>
            </li>
            <li class="nav-communications">
                <a href="../communications/index.html">Communications</a>
            </li>
            <li class="nav-teaching">
                <a href="../cours-de-m2-data-sciences/index.html">Teaching</a>
            </li>
</ul>

        </div>
    </div>
</header>
        <main id="site-main" class="site-main">
            <div id="swup" class="transition-fade">

            
<article class="post-content post no-image no-image">

    <header class="post-content-header">
        <h1 class="post-content-title">About</h1>
    </header>


    <div class="post-content-body">
        <p>I am currently completing my Ph.D. in Natural Language Processing at <a href="https://u-paris.fr/en/">Paris University</a> in a joint program sponsored by <a href="https://www.quantmetry.com/">Quantmetry</a>. I graduated from <a href="https://www.ensta-paris.fr/">ENSTA Paris</a> in mathematical optimization and obtained a Master (M2) from <a href="https://www.polytechnique.edu/en">Ecole Polytechnique</a> in Data Science.</p><p>My research focuses on building standalone sentence embeddings. I examine sentence encoder architectures, as well as training and evaluation methods. I contribute to the open-source community with the creation of datasets and implementation of deep learning models. I specifically contribute to resources for French.</p><h1 id="research-statement">Research Statement</h1><p>My primary research interest lies in the area of natural language processing (NLP) where I work on architectures of computational models to produce sentence embeddings. Building standalone sentence embeddings is specifically hard, as an infinite number of valid sentences exist. However,  compositional semantics state that the meaning of a phrase is determined by combining the meanings of its subphrases, using rules. Models, therefore, need to compose text units, given a syntactic structure, into global semantic embeddings. Enabled by my lab's interdisciplinary research in both computational and theoretical linguistics, I include linguistic biases into neural networks. I then analyze how their inner sequence of compositions compare with linguistic theory and the gain enabled by such biases. This aspect of my work is detailed in section §1. I am also motivated by academics and industry applications of sentence embeddings, such as search engines or text mining. As detailed in §2, I train such models at scale to obtain state-of-the-art results in the domain of sentence embeddings and language modeling. I share a large portion of this work as open-source contributions, ready to use for real-world applications. In section §3, I detail how I intend to pursue my research to increase the model’s controllability and intelligibility. I aim at integrating other kinds of biases into machine learning models such as combining symbolic and statistic approaches.</p><h3 id="1-toward-integrating-linguistic-biases-into-neural-networks">1. Toward integrating linguistic biases into neural networks</h3><p>My first line of research aims at improving the compositional properties of machine learning models and their ability to generalize outside their training domain. I aim to integrate the recursive property of language within neural models. I design and analyze architectures based on linguistic theory.</p><p>Jointly learning model structure and compositional operations I focus on tree-structured neural networks, which naturally encode the structure of language. For each sentence, the network computes text units following a syntactic tree, starting from the leaf nodes, up to the root. However, such models suffer from practical constraints that limit their application. In particular, tree-based models not only require raw text as input but also the sentence structure in the form of a parse tree. Such structure may be tedious to obtain as it requires manual annotations and external parsers. To overcome such limitations, I formulated a novel tree-based model that learns its composition function together with its structure <a>[1]</a><br>). The model includes two modules, a biaffine graph parser, and a Tree-LSTM. The parsing and the composition functions are explicitly connected and, therefore, learned jointly. The method differs from previous work as the representation is not computed from the whole forest of potential trees. Moreover, training the full model directly does not require supervision from a parsing objective. The model outperforms tree-based models relying on external parsers on downstream tasks. In some configurations, it is even competitive with BERT-base model.</p><p>Studying shallow structure in transformer models Recent transformer architectures have gained increased popularity within the community. Contrary to tree-based models, they do not need carefully hand-annotated data to be trained. On the other hand, as many results suggest, these new models acquire some sort of tree structure. Transformers update each token hidden simultaneously through a fixed number of layers. Yet the role of these layers and how they process information is not fully understood. I formulate the hypothesis that the distinct layers do not encode specific surface, syntactic nor semantic functions but rather that such information emerges through the iterative application of layers. To better study the transformation of token representations across layers, I proposed a variant of ALBERT <a>[2]</a>. This model implements the key specificity of weights tying across layers, but also dynamically adapts the number of layers applied to each token. I analyze token transformation across the network depth. In particular, I study how iterations are distributed given the token dependency types. I showed that tokens do not require the same amount of iterations and that difficult or crucial tokens for the task are subject to more iterations.</p><p>Characterizing compositional properties of neural architectures While transformers show outstanding performances on many NLP benchmarks, they also have some linguistic limitations. In particular, regarding their ability to generalize outside their training range and to learn elementary composition rules. The benchmark COGS <a>[3]</a> for example highlights deep learning models struggle to generalize to longer sequences or sentences with deeper level of recursion than seen during training. Following my work on integrating structure into neural architecture, I aim at better characterizing how the model structure may affect their degree of compositionality. This work is currently in an experimentation phase. I am building an evaluation setup with arithmetic expressions containing specific properties. I train various models on specific subsets and observe how models generalize outside their domain. In particular, I compare models relying on different degrees of structure constraints such as sequential, recursive, or unstructured models.</p><h3 id="2-training-language-models-at-scale">2. Training language models at scale</h3><p>My second line of research focuses on training and sharing models at scale. Indeed, the preparation of massive corpora, the training, and the use of large architectures are key for the performance of such models. Moreover, specific behaviors and linguistic properties deeply depend on the scale.</p><p>Training sentence embedding models using a discriminative objective Inspired from linguistic insights, I assume structure is crucial to building consistent representations. I indeed expect sentence meaning to be a function of both syntax and semantic aspects. In that regard, I proposed a self-supervised method that builds sentence embeddings from the combination of diverse explicit syntactic structures of a sentence <a>[4]</a>. The novelty consists in jointly learning structured models in a contrastive multi-view framework that induces an explicit interaction between models during the training phase. I pre-trained various models using a contrastive objective with a 40 million sentences corpus. I then evaluate my models on sentence embedding benchmarks and obtain state-of-the-art results. In particular on tasks that are expected, by hypothesis, to be more sensitive to sentence structure. From a practical point of view, implementing tree-structured models can be hard. I open-sourced the code I developed for recursive models under a library called PyTree . The library was distinguished and listed among the winners of the PyTorch Hackathon 2021. Motivated to share state-of-the-art models, I also participated in a hackathon  to develop, train and release large sentence embeddings models. We used a similar contrastive objective and trained models on a 1 billion sentences corpora. We developed specific evaluation benchmarks for sentence embeddings and obtained state-of-the-art results. Our project was among the winners of the competition and received an honorable mention.</p><p>Training the first large language model for French using a generative objective As observed in <a>[5]</a>, deep neural networks have shocking grammatical competencies. For example, GPT-2 generates correct text with plural and long-distance agreement despite any prior linguistic knowledge. Such agreements are determined by abstract structures and not just linear order of words. Surprisingly, models can learn such specific linguistic patterns (subject-verb, noun-adverb, verb-verb) with no prior information about linguistic theory. Within my laboratory, I led the project to train the first large language model in French <a>[6]</a>. We obtained a dedicated computation grant on public French HPC computer Jean Zay. The model, equivalent to GPT-2 in English, contains more than 1 billion parameters. We built a dedicated training corpus and parallelized the training between multiple nodes and compute units. I am particularly proud of this project, as we contributed to the resources available in French. We released the model in Open-Source for research and business application purposes .</p><h3 id="3-future-research-directions">3. Future Research Directions</h3><p>In my opinion, recent advances in NLP have opened up new and exciting applications. Yet, current architectures lack control and intelligibility properties. When using models for real-world applications, it is hard to avoid, let alone explain, unwanted behavior. In that perspective, I intend to pursue my research in the direction of integrating linguistic or formal theory into machine learning models and training such architectures at scale. In the following sections, I identify two main directions that may be of particular interest to increase the models' robustness to generalization outside their domains and provide efficient tools for improving intelligibility and control over language models.</p><p>Integrating symbolic and logic bias into language models Symbolic AI typically encodes knowledge using explicit rules. These systems may require extensive feature engineering to describe individual elements, but they are very effective at explaining how to compose them. By hard integrating composition rules, they are naturally more resilient to out-of-domain generalization. Combining symbolic systems and deep learning representation methods is an active subject of research. For example, to combine object recognition and reasoning abilities using generation of symbolic programs or by integrating logic into neural networks. Following my work to integrate structure constraints in neural networks, I aim to integrate logic constraints into architectures. In my opinion, such approach complements methods for intelligibility in deep neural networks. Indeed, we do not attempt to explain models afterward but rather try to constrain their architectures to provide more explicit or readable transformation sequences. Such approach may also enhance models out of domain generalization properties by providing new regularization methods.</p><p>Toward controllable text generation Language models currently integrate knowledge within the network hidden states. For natural language generation, we may observe unwanted behavior such as hallucination or factually incorrect statements. While such models may be used in original setups such as few-shots or zero-shot learning, they still lack controllable properties. Indeed, the generated statements depend on the architecture, the data used for pre-training, or the prompt used during the task. Many works focus on prompt engineering to control the model afterward. But we may also consider refining the architecture, as stated previously, or the form of the data used to infuse knowledge into the network. For example by using specific memory structures or by using conditional variables. Better controlling language models' generative properties may also help us reduce their exponentially growing size by identifying redundant parameters. In general, such properties appear critical to building consistent and robust systems for real-world applications.</p><h3 id="references">References</h3><p><a>[1]</a> Antoine Simoulin, Benoît Crabbé: Unifying parsing and tree-structured models for generating sentence semantic representations. CoRR abs (2021)</p><p><a>[2]</a> Antoine Simoulin, Benoît Crabbé: How Many Layers and Why? An Analysis of the Model Depth in Transformers. ACL (student) 2021: 221-228</p><p><a>[3]</a> Najoung Kim, Tal Linzen: COGS: A Compositional Generalization Challenge Based on Semantic Interpretation. EMNLP (1) 2020: 9087-9105</p><p><a>[4]</a> Antoine Simoulin, Benoît Crabbé: Contrasting distinct structured views to learn sentence embeddings. EACL (Student Research Workshop) 2021: 71-79</p><p><a>[5]</a> Tal Linzen, Marco Baroni: Syntactic Structure from Deep Learning. CoRR abs/2004.10827 (2020)</p><p><a>[6]</a> Antoine Simoulin, Benoît Crabbé: Generative Pre-trained Transformer in ______ French. TALN (1) 2021: 246-255</p>
    </div>

</article>



            </div>
        </main>

        <footer class="site-foot">
    <div class="site-foot-container">
        <div class="site-foot-left">
            © 2022 <a href="../index.html"></a> — Published with <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
        </div>
        <div class="site-foot-right">
            <ul class="nav">
            <li class="nav-twitter">
                <a href="https://twitter.com/_Antwane_" class="social-link">
                    <svg width="25" height="25" role="img" aria-label="twitter icon">
                        <use xlink:href="#twitter"></use>
                    </svg>
                </a>
            </li>
            <li class="nav-linkedin">
                <a href="https://www.linkedin.com/in/antoine-simoulin-a00960b1/" class="social-link">
                    <svg width="25" height="25" role="img" aria-label="linkedin icon">
                        <use xlink:href="#linkedin"></use>
                    </svg>
                </a>
            </li>
            <li class="nav-github">
                <a href="https://github.com/AntoineSimoulin" class="social-link">
                    <svg width="25" height="25" role="img" aria-label="github icon">
                        <use xlink:href="#github"></use>
                    </svg>
                </a>
            </li>
            <li class="nav-dblp">
                <a href="https://dblp.org/pid/211/7662.html" class="social-link">
                    <svg width="25" height="25" role="img" aria-label="dblp icon">
                        <use xlink:href="#dblp"></use>
                    </svg>
                </a>
            </li>
            <li class="nav-acl">
                <a href="https://aclanthology.org/people/a/antoine-simoulin/" class="social-link">
                    <svg width="25" height="25" role="img" aria-label="acl icon">
                        <use xlink:href="#acl"></use>
                    </svg>
                </a>
            </li>
</ul>

        </div>
    </div>

</footer>
    </div>


    <script src="../assets/built/swup.js?v=1ef6d3fffe" data-swup-ignore-script></script>
    <script src="../assets/built/infinitescroll.js?v=1ef6d3fffe" data-swup-ignore-script></script>
    <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous">
    </script>

    <div id="swup">
        <script>
    var images = document.querySelectorAll('.kg-gallery-image img');
    images.forEach(function (image) {
        var container = image.closest('.kg-gallery-image');
        var width = image.attributes.width.value;
        var height = image.attributes.height.value;
        var ratio = width / height;
        container.style.flex = ratio + ' 1 0%';
    })
</script>


        <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    }
</script>
    </div>

    <script data-swup-ignore-script>
        $(document).ready(function () {
            // Mobile Menu Trigger
            $('.nav-burger').click(function () {
                $('body').toggleClass('site-head-open');
            });
        });

        // Initiate Swup transitions
        var swup = new Swup({
            plugins: [new SwupHeadPlugin(), new SwupScriptsPlugin()],
        });
        document.addEventListener('swup:contentReplaced', event => {
            window.scrollTo(0, 0);
            initInfiniteScroll(window, document);
            $('body').removeClass('site-head-open');
        });
    </script>

</body>
</html>